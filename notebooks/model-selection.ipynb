{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbb81c7",
   "metadata": {},
   "source": [
    "# Model selection and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099209db",
   "metadata": {},
   "source": [
    "__Notes__\n",
    "\n",
    "1. Create set of rules for what reviews to accept or throw out\n",
    "    1. Min or max number of words\n",
    "    1. Include URLs or not\n",
    "    1. Emojis\n",
    "    1. Eliminate stop words\n",
    "    \n",
    "1. Model selection\n",
    "    1. CNN for usefulness\n",
    "    1. Sample from useful reviews only\n",
    "    1. Setup T5 transfering learning model to generate \"justification\" text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fed62e",
   "metadata": {},
   "source": [
    "## 0.0 Notebook setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f083e6",
   "metadata": {},
   "source": [
    "### X.X Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aX0IHvqPrB9h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aX0IHvqPrB9h",
    "outputId": "3b24dbf0-24a8-4a66-c696-d99b78bf5f83"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9vnzzqbdraH_",
   "metadata": {
    "id": "9vnzzqbdraH_"
   },
   "outputs": [],
   "source": [
    "! mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0A4JchtFtgsT",
   "metadata": {
    "id": "0A4JchtFtgsT"
   },
   "outputs": [],
   "source": [
    "! cp \"/content/gdrive/My Drive/nlp-book-reviews/data/review_samples.csv\" ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ncWfMYNDwGnj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ncWfMYNDwGnj",
    "outputId": "1874e24b-22c3-4598-e8e7-1b1d8f3d5ec4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VIpdJLpswGLc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIpdJLpswGLc",
    "outputId": "9cd6252f-cd70-4444-c1d0-9994c3a964bc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5bb2cb",
   "metadata": {},
   "source": [
    "### X.X Local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0320af",
   "metadata": {
    "id": "3f0320af"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abac3ad",
   "metadata": {
    "id": "3abac3ad"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency, norm, skew, kurtosis\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from common import vocabulary\n",
    "\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "# import spacy\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from collections import Counter\n",
    "# from wordcloud import WordCloud\n",
    "from unicodedata import normalize\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, Dense, LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbfaff0",
   "metadata": {
    "id": "6bbfaff0"
   },
   "source": [
    "## 4.0 Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ac3c5",
   "metadata": {
    "id": "cb8ac3c5"
   },
   "source": [
    "### 4.1 Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ab981",
   "metadata": {
    "id": "523ab981"
   },
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "max_len = 200\n",
    "\n",
    "reviews = review_samples['reviewText']\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "reviews = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "labels = review_samples['useful']\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=2, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a255b76",
   "metadata": {
    "id": "9a255b76"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6556cac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6556cac",
    "outputId": "c0cc4fc5-5efd-414e-84e5-843388f7c273"
   },
   "outputs": [],
   "source": [
    "# Specify model hyperparameters.\n",
    "epochs = 5\n",
    "dropout_rate = 0.7\n",
    "num_classes = len(np.unique(labels, axis=0))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 40, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(20, dropout=dropout_rate)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4bcaea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb4bcaea",
    "outputId": "2a67e33e-8154-4e8e-a731-5c22dd3a5d18"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf8b78",
   "metadata": {
    "id": "0ccf8b78"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.argmax(axis=1)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d45011",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "e7d45011",
    "outputId": "1b06a1ba-e70b-4690-a930-446cfc20371b"
   },
   "outputs": [],
   "source": [
    "sentiment = ['Not Useful', 'Useful']\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_true,\n",
    "                                        y_pred,\n",
    "                                        normalize='all',\n",
    "                                        display_labels=sentiment\n",
    "                                       )\n",
    "\n",
    "plt.title(\"Confusion Matrix of Review Usefulness\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59598ada",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "59598ada",
    "outputId": "836ac1a9-e05f-4f18-dc71-8ae650a2baa6"
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b299f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "f4b299f7",
    "outputId": "1a97adbf-5723-4c75-fcca-65c65e267745"
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['i really loved how the technician helped me with the issue that i had'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73365d",
   "metadata": {
    "id": "ae73365d"
   },
   "source": [
    "### 4.X BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8aa49",
   "metadata": {
    "id": "4ce8aa49"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a87b0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235,
     "referenced_widgets": [
      "6f685d96196f4e3abb0c6e08fe5aee92",
      "43cf49055dbc4842a1a46fc9df8ee932",
      "1119511508184c45b3441c84e44942cd",
      "633a6bc9034b4b52a4f5ceb0e213349c",
      "0860a168404f4f1d96b8ef6fe1acc061",
      "90b0e5e62e244172a59225587f5d084d",
      "8730c717632a468ea50e5255f8946e82",
      "13ee2169a03e444d91689b505649cbed",
      "b5ef1a6d67c0431cb30ebacd296e526a",
      "3d1196f554bd4a28a1aff86ea032d3c3",
      "02603486a9ae4386a9e231110b1245de",
      "7c7d53c256a4431d987de57f0287e0a1",
      "17d32e41a19447f0a17789d142904586",
      "36ce42e417af40bea840a7df83fcaf31",
      "ef8bbf83b5224efcaf46b479cd6d93e6",
      "d959e779476a4f79b5d8f06703621c85",
      "8f5e6ca0bb8343bf8ea4d9a2e656166c",
      "f5824d140287416c808bcbc7a24ac124",
      "c89b492faca7411a83482f65df06c45d",
      "2d4ab0bd0aa54ce89f8a43d83384ee48",
      "9d778e4100294381ab56dd894c5f1472",
      "6619148b460b45aba80f6171b73d61df",
      "62f2609230e44f86a4a48d96a2460215",
      "4f5cab468d1d41999f10657f7b79b50a",
      "0d77126e84d3477daaf9cb466e274f55",
      "d98db63696bb4737b89ef6e440b957f3",
      "a83f5aa55c4a433283a0190692243bb4",
      "1c5d31b8ad664bf187f96b516c4024ee",
      "11fbb752d78c40f98b730776631b5693",
      "48ec534e41df4b6cb8ab76ad6ee7911d",
      "a01f66f7cc90408892437f2ed1d616f8",
      "96720bada37044a18dd9b657820174cd",
      "949eb92a430649d18999712c04a90a02",
      "c351d8ef2d754bb7b2cab3ec781e108c",
      "64799d9b7ea74fb2b23f1b811ff7c4e6",
      "c995702a012742428f0ff4359d32a6c2",
      "af19196510b74527a5bf5d8f98850246",
      "020f62f332c042e79754e5f8ed44bff5",
      "bc3dd61b8c5d4cfca12cb315bb8378f7",
      "82dd2fae32844f77a1c7299eb2a8fdae",
      "17a01879195744809f0f4bf86af8c8e9",
      "1a08d61e88e047c488b396bbd45049a2",
      "5031e91548e64613a736ee45e1b41b9d",
      "880b22c5d3f94d878d0b457a4119bdb9"
     ]
    },
    "id": "05a87b0c",
    "outputId": "f4bb8708-03ba-4a10-cfaa-bd95a403210a"
   },
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e8d32",
   "metadata": {
    "id": "1a4e8d32"
   },
   "outputs": [],
   "source": [
    "X, y = review_samples['reviewText'], review_samples['useful']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
    "\n",
    "train = pd.DataFrame([X_train, y_train]).T\n",
    "dev = pd.DataFrame([X_dev, y_dev]).T\n",
    "test = pd.DataFrame([X_test, y_test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae84fa",
   "metadata": {
    "id": "f0ae84fa"
   },
   "outputs": [],
   "source": [
    "def convert_data_to_examples(data, DATA_COLUMN, LABEL_COLUMN): \n",
    "    examples = data.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                 text_a = x[DATA_COLUMN], \n",
    "                                                 text_b = None,\n",
    "                                                 label = x[LABEL_COLUMN]\n",
    "                                                ),\n",
    "                          axis = 1\n",
    "                         )\n",
    "  \n",
    "    return examples\n",
    "\n",
    "\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(e.text_a,\n",
    "                                           add_special_tokens=True,\n",
    "                                           max_length=max_length, # truncates if len(s) > max_length\n",
    "                                           return_token_type_ids=True,\n",
    "                                           return_attention_mask=True,\n",
    "                                           pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "                                           truncation=True\n",
    "                                          )\n",
    "\n",
    "        input_ids = input_dict[\"input_ids\"]\n",
    "        token_type_ids = input_dict[\"token_type_ids\"] \n",
    "        attention_mask = input_dict['attention_mask']\n",
    "\n",
    "        features.append(InputFeatures(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask,\n",
    "                                      token_type_ids=token_type_ids,\n",
    "                                      label=e.label\n",
    "                                     )\n",
    "                       )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield ({\"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                   },\n",
    "                   f.label,\n",
    "                  )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen,\n",
    "                                          ({\"input_ids\": tf.int32,\n",
    "                                            \"attention_mask\": tf.int32,\n",
    "                                            \"token_type_ids\": tf.int32\n",
    "                                           },\n",
    "                                           tf.int64\n",
    "                                          ),\n",
    "                                          ({\"input_ids\": tf.TensorShape([None]),\n",
    "                                            \"attention_mask\": tf.TensorShape([None]),\n",
    "                                            \"token_type_ids\": tf.TensorShape([None]),\n",
    "                                           },\n",
    "                                           tf.TensorShape([]),\n",
    "                                          ),\n",
    "                                         )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb02e2",
   "metadata": {
    "id": "dbcb02e2"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'reviewText'\n",
    "LABEL_COLUMN = 'useful'\n",
    "\n",
    "train_InputExamples = convert_data_to_examples(test, DATA_COLUMN, LABEL_COLUMN)\n",
    "dev_InputExamples = convert_data_to_examples(test, DATA_COLUMN, LABEL_COLUMN)\n",
    "test_InputExamples = convert_data_to_examples(test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "dev_data = convert_examples_to_tf_dataset(list(dev_InputExamples), tokenizer)\n",
    "dev_data = dev_data.batch(32)\n",
    "\n",
    "test_data = convert_examples_to_tf_dataset(list(test_InputExamples), tokenizer)\n",
    "test_data = test_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a678b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c6a678b",
    "outputId": "c754e642-0fc5-4c40-ae12-8ab42e930d40"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "model.fit(train_data, epochs=2, validation_data=dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NNv0TS7YzwRr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNv0TS7YzwRr",
    "outputId": "cf3bb4a3-9508-4366-db53-42dffbeccee8"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x1E-9yN20BRY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1E-9yN20BRY",
    "outputId": "eb9fe050-f959-4c3a-a905-cfd6491f4e41"
   },
   "outputs": [],
   "source": [
    "# test_sentence = \"Golf tips magazine is one of the most aptly titled magazines available today. \\\n",
    "#                  It is chock full of heavily-illustrated exercises and other tips for serious golfers to improve their games.\\\n",
    "#                  \\nThere are a number of golf magazines on the newstand today. Most of them rely on a lifestyle format relying \\\n",
    "#                  more heavily on lifestyle-type stories about today's hot golfers and the courses they play. Most of them include \\\n",
    "#                  a brief game tip or two but seem aimed at the casual duffer.\\nGolf Tips, on the other hand, features almost nothing \\\n",
    "#                  but heavily illustrated tips on improving one's swing, eliminating mistakes and putting better. It also is heavily \\\n",
    "#                  loaded with features on the latest equipment technology. Each issue also seems to feature an article on the technical \\\n",
    "#                  aspects of a selected major golf course. The articles are written with terminology that serious golfers will understand \\\n",
    "#                  but that may confuse the weekend player. Judging by the amount of advertising in its early issues, this magazine also \\\n",
    "#                  appears to be financially healthy.\\nIf a reader wants to read about the PGA Tour's superstars, this magazine is not for \\\n",
    "#                  him/her. But if s/he plans to be one of those superstars, Golf Tips is a good match.\"\n",
    "\n",
    "test_sentence = \"The information is interesting and fun, the writing is superb, and the writers and editors have a great sense of humor. \\\n",
    "                 I just wish it came out more often!\"\n",
    "\n",
    "predict_input = tokenizer.encode(test_sentence,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\"\n",
    "                                 )\n",
    "tf_output = model.predict(predict_input)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Not Useful','Useful'] #(0:Not Useful, 1:Useful)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label = label.numpy()\n",
    "print(labels[label[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549dde8",
   "metadata": {
    "id": "e549dde8"
   },
   "source": [
    "### 4.X Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf288c",
   "metadata": {
    "id": "1bcf288c"
   },
   "outputs": [],
   "source": [
    "# Specify model hyperparameters\n",
    "epochs = 5\n",
    "embed_dim = 5\n",
    "num_filters = [2, 2, 2]\n",
    "kernel_sizes = [2, 3, 4]\n",
    "dense_layer_dims = [10, 4]\n",
    "dropout_rate = 0.7\n",
    "num_classes = len(np.unique(labels, axis=0))\n",
    "\n",
    "# Construct the convolutional neural network.\n",
    "# The form of each keras layer function is as follows:\n",
    "#    result = keras.layers.LayerType(arguments for the layer)(layer(s) it should use as input)\n",
    "# concretely,\n",
    "#    this_layer_output = keras.layers.Dense(100, activation='relu')(prev_layer_vector)\n",
    "# performs this_layer_output = relu(prev_layer_vector x W + b) where W has 100 columns.\n",
    "\n",
    "# Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
    "# In our case, we are accepting a list of wordids (padded out to max_len).\n",
    "wordids = keras.layers.Input(shape=(max_len,))\n",
    "\n",
    "# Embed the wordids.\n",
    "# Recall, this is just a mathematically equivalent operation to a linear layer and a one-hot\n",
    "h = keras.layers.Embedding(ds.vocab.size, embed_dim, input_length=max_len)(wordids)\n",
    "\n",
    "# Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
    "# With the default hyperparameters, we construct 2 filters each of size 2, 3, 4.  As in the image above, each filter\n",
    "# is wide enough to span the whole word embedding (this is why the convolution is \"1d\" as seen in the\n",
    "# function name below).\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "# Concat the feature maps from each different size.\n",
    "h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "\n",
    "# Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values\n",
    "# in the vector.\n",
    "# See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf for details.\n",
    "h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Add a fully connected layer for each dense layer dimension in dense_layer_dims.\n",
    "### END YOUR CODE\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a7653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
