{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20bb2820",
   "metadata": {},
   "source": [
    "# Model selection and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca5f9b",
   "metadata": {},
   "source": [
    "__Notes__\n",
    "\n",
    "1. Create set of rules for what reviews to accept or throw out\n",
    "    1. Min or max number of words\n",
    "    1. Include URLs or not\n",
    "    1. Emojis\n",
    "    1. Eliminate stop words\n",
    "    \n",
    "1. Model selection\n",
    "    1. CNN for usefulness\n",
    "    1. Sample from useful reviews only\n",
    "    1. Setup T5 transfering learning model to generate \"justification\" text\n",
    "\n",
    "- LSTM is good\n",
    "    - You can add in an attention layer because it can look backwards and learn what words were more important than others\n",
    "- CNN is good for sentence classification \n",
    "    - With a fully connected Dense layer it is not good at generalizing\n",
    "- BERT is good because of its self-attention mechanism where every word looks at every other layer\n",
    "    - Use the CLS token as the only thing you feed into the output layer that is going to make the prediction on the classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c4870",
   "metadata": {},
   "source": [
    "## 0.0 Notebook setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e531ea",
   "metadata": {},
   "source": [
    "### X.X Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aX0IHvqPrB9h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aX0IHvqPrB9h",
    "outputId": "3b24dbf0-24a8-4a66-c696-d99b78bf5f83"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9vnzzqbdraH_",
   "metadata": {
    "id": "9vnzzqbdraH_"
   },
   "outputs": [],
   "source": [
    "! mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0A4JchtFtgsT",
   "metadata": {
    "id": "0A4JchtFtgsT"
   },
   "outputs": [],
   "source": [
    "! cp \"/content/gdrive/My Drive/nlp-book-reviews/data/review_samples.csv\" ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ncWfMYNDwGnj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ncWfMYNDwGnj",
    "outputId": "1874e24b-22c3-4598-e8e7-1b1d8f3d5ec4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VIpdJLpswGLc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIpdJLpswGLc",
    "outputId": "9cd6252f-cd70-4444-c1d0-9994c3a964bc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631b809",
   "metadata": {},
   "source": [
    "### X.X Local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f0320af",
   "metadata": {
    "id": "3f0320af"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abac3ad",
   "metadata": {
    "id": "3abac3ad"
   },
   "outputs": [],
   "source": [
    "# Statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency, norm, skew, kurtosis\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model selection and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Model building\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, Dense, Flatten, LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# BERT-specific\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768f4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_samples = pd.read_csv('../data/review_samples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ffdf0f",
   "metadata": {},
   "source": [
    "## X.X Usefulness language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ac3c5",
   "metadata": {
    "id": "cb8ac3c5"
   },
   "source": [
    "### X.X Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "523ab981",
   "metadata": {
    "id": "523ab981"
   },
   "outputs": [],
   "source": [
    "num_words = None\n",
    "oov_token = '<UNK>'\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "reviews = review_samples['reviewText']\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "\n",
    "max_words = len(tokenizer.word_index)\n",
    "max_len = max([len(x) for x in sequences])\n",
    "reviews = pad_sequences(sequences, padding=pad_type, truncating=trunc_type, maxlen=max_len)\n",
    "\n",
    "labels = review_samples['useful']\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=2, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a255b76",
   "metadata": {
    "id": "9a255b76"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6556cac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6556cac",
    "outputId": "c0cc4fc5-5efd-414e-84e5-843388f7c273"
   },
   "outputs": [],
   "source": [
    "# Specify model hyperparameters.\n",
    "epochs = 5\n",
    "dropout_rate = 0.7\n",
    "num_classes = len(np.unique(labels, axis=0))\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 40, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(20, dropout=dropout_rate)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb4bcaea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb4bcaea",
    "outputId": "2a67e33e-8154-4e8e-a731-5c22dd3a5d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 2s 18ms/step - loss: 0.6873 - accuracy: 0.6026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6873011589050293, 0.6026119589805603]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ccf8b78",
   "metadata": {
    "id": "0ccf8b78"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.argmax(axis=1)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d45011",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "e7d45011",
    "outputId": "1b06a1ba-e70b-4690-a930-446cfc20371b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment = ['Not Useful', 'Useful']\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_true,\n",
    "                                        y_pred,\n",
    "                                        normalize='all',\n",
    "                                        display_labels=sentiment\n",
    "                                       )\n",
    "\n",
    "plt.title(\"Confusion Matrix of Review Usefulness\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d2af57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.67      0.63       555\n",
      "           1       0.60      0.53      0.56       517\n",
      "\n",
      "    accuracy                           0.60      1072\n",
      "   macro avg       0.60      0.60      0.60      1072\n",
      "weighted avg       0.60      0.60      0.60      1072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59598ada",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "59598ada",
    "outputId": "836ac1a9-e05f-4f18-dc71-8ae650a2baa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Useful'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b299f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "f4b299f7",
    "outputId": "1a97adbf-5723-4c75-fcca-65c65e267745"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Useful'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['i really loved how the technician helped me with the issue that i had'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73365d",
   "metadata": {
    "id": "ae73365d"
   },
   "source": [
    "### X.X BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e8d32",
   "metadata": {
    "id": "1a4e8d32"
   },
   "outputs": [],
   "source": [
    "X, y = review_samples['reviewText'], review_samples['useful']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
    "\n",
    "train = pd.DataFrame([X_train, y_train]).T\n",
    "dev = pd.DataFrame([X_dev, y_dev]).T\n",
    "test = pd.DataFrame([X_test, y_test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0ae84fa",
   "metadata": {
    "id": "f0ae84fa"
   },
   "outputs": [],
   "source": [
    "def convert_data_to_examples(data, DATA_COLUMN, LABEL_COLUMN): \n",
    "    examples = data.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                 text_a = x[DATA_COLUMN], \n",
    "                                                 text_b = None,\n",
    "                                                 label = x[LABEL_COLUMN]\n",
    "                                                ),\n",
    "                          axis = 1\n",
    "                         )\n",
    "  \n",
    "    return examples\n",
    "\n",
    "\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(e.text_a,\n",
    "                                           add_special_tokens=True,\n",
    "                                           max_length=max_length, # truncates if len(s) > max_length\n",
    "                                           return_token_type_ids=True,\n",
    "                                           return_attention_mask=True,\n",
    "                                           pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "                                           truncation=True\n",
    "                                          )\n",
    "\n",
    "        input_ids = input_dict[\"input_ids\"]\n",
    "        token_type_ids = input_dict[\"token_type_ids\"] \n",
    "        attention_mask = input_dict['attention_mask']\n",
    "\n",
    "        features.append(InputFeatures(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask,\n",
    "                                      token_type_ids=token_type_ids,\n",
    "                                      label=e.label\n",
    "                                     )\n",
    "                       )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield ({\"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                   },\n",
    "                   f.label,\n",
    "                  )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen,\n",
    "                                          ({\"input_ids\": tf.int32,\n",
    "                                            \"attention_mask\": tf.int32,\n",
    "                                            \"token_type_ids\": tf.int32\n",
    "                                           },\n",
    "                                           tf.int64\n",
    "                                          ),\n",
    "                                          ({\"input_ids\": tf.TensorShape([None]),\n",
    "                                            \"attention_mask\": tf.TensorShape([None]),\n",
    "                                            \"token_type_ids\": tf.TensorShape([None]),\n",
    "                                           },\n",
    "                                           tf.TensorShape([]),\n",
    "                                          ),\n",
    "                                         )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb02e2",
   "metadata": {
    "id": "dbcb02e2"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'reviewText'\n",
    "LABEL_COLUMN = 'useful'\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_InputExamples = convert_data_to_examples(test, DATA_COLUMN, LABEL_COLUMN)\n",
    "dev_InputExamples = convert_data_to_examples(test, DATA_COLUMN, LABEL_COLUMN)\n",
    "test_InputExamples = convert_data_to_examples(test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "dev_data = convert_examples_to_tf_dataset(list(dev_InputExamples), tokenizer)\n",
    "dev_data = dev_data.batch(32)\n",
    "\n",
    "test_data = convert_examples_to_tf_dataset(list(test_InputExamples), tokenizer)\n",
    "test_data = test_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a678b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c6a678b",
    "outputId": "c754e642-0fc5-4c40-ae12-8ab42e930d40"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Specify model hyperparameters.\n",
    "epochs = 5\n",
    "dropout_rate = 0.7\n",
    "num_classes = len(np.unique(labels, axis=0))\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5,\n",
    "                                                 epsilon=1e-08,\n",
    "                                                 clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "             )\n",
    "\n",
    "model.fit(train_data, epochs=2, validation_data=dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NNv0TS7YzwRr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNv0TS7YzwRr",
    "outputId": "cf3bb4a3-9508-4366-db53-42dffbeccee8"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x1E-9yN20BRY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1E-9yN20BRY",
    "outputId": "eb9fe050-f959-4c3a-a905-cfd6491f4e41"
   },
   "outputs": [],
   "source": [
    "# test_sentence = \"Golf tips magazine is one of the most aptly titled magazines available today. \\\n",
    "#                  It is chock full of heavily-illustrated exercises and other tips for serious golfers to improve their games.\\\n",
    "#                  \\nThere are a number of golf magazines on the newstand today. Most of them rely on a lifestyle format relying \\\n",
    "#                  more heavily on lifestyle-type stories about today's hot golfers and the courses they play. Most of them include \\\n",
    "#                  a brief game tip or two but seem aimed at the casual duffer.\\nGolf Tips, on the other hand, features almost nothing \\\n",
    "#                  but heavily illustrated tips on improving one's swing, eliminating mistakes and putting better. It also is heavily \\\n",
    "#                  loaded with features on the latest equipment technology. Each issue also seems to feature an article on the technical \\\n",
    "#                  aspects of a selected major golf course. The articles are written with terminology that serious golfers will understand \\\n",
    "#                  but that may confuse the weekend player. Judging by the amount of advertising in its early issues, this magazine also \\\n",
    "#                  appears to be financially healthy.\\nIf a reader wants to read about the PGA Tour's superstars, this magazine is not for \\\n",
    "#                  him/her. But if s/he plans to be one of those superstars, Golf Tips is a good match.\"\n",
    "\n",
    "test_sentence = \"The information is interesting and fun, the writing is superb, and the writers and editors have a great sense of humor. \\\n",
    "                 I just wish it came out more often!\"\n",
    "\n",
    "predict_input = tokenizer.encode(test_sentence,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\"\n",
    "                                 )\n",
    "tf_output = model.predict(predict_input)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Not Useful','Useful'] #(0:Not Useful, 1:Useful)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label = label.numpy()\n",
    "print(labels[label[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549dde8",
   "metadata": {
    "id": "e549dde8"
   },
   "source": [
    "### X.X Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83573cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ac9522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirming as of April 2010 - The magazines do NOT come packaged with the software CD's as pictured on the product page, nor does the description disclaim that this is some kind no-CD subscription of the magazine.\n",
      "\n",
      "The reviews on the ridiculous time frames in which you receive your first issue are, unfortunately, true as well. The description claims 6-10 weeks, but it could take several months longer.\n",
      "\n",
      "Falsely advertised; buy directly through Maximum PC or a more reliable source where you'll actually get all 12 months with all 12 software discs, rather than 6-10 issues with no discs.\n",
      "\n",
      "This product is NOT the product described. It's as simple as that.\n",
      "\n",
      "confirming as of april 2010 the magazines do not come packaged with the software cd 's as pictured on the product page , nor does the description disclaim that this is some kind no cd subscription of the magazine the reviews on the ridiculous time frames in which you receive your first issue are , unfortunately , true as well the description claims 6 10 weeks , but it could take several months longer falsely advertised buy directly through maximum pc or a more reliable source where you 'll actually get all 12 months with all 12 software discs , rather than 6 10 issues with no discs this product is not the product described it 's as simple as that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_samples['reviewProcessed'] = review_samples['reviewText'].apply(lambda row: clean_str(row))\n",
    "\n",
    "# Example review\n",
    "print(review_samples[\"reviewText\"].iloc[1])\n",
    "print(\"\")\n",
    "print(review_samples[\"reviewProcessed\"].iloc[1])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c4f3afaf",
   "metadata": {
    "id": "523ab981"
   },
   "outputs": [],
   "source": [
    "num_words = None\n",
    "oov_token = '<UNK>'\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "reviews = review_samples['reviewText']\n",
    "tokenizer = Tokenizer(num_words=top_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "\n",
    "# top_words = len(tokenizer.word_index)\n",
    "# max_words = max([len(x) for x in sequences])\n",
    "top_words = 7000\n",
    "max_words = 450\n",
    "\n",
    "reviews = pad_sequences(sequences, padding=pad_type, truncating=trunc_type, maxlen=max_words)\n",
    "\n",
    "review_samples['useful'] = review_samples['useful'].astype(int)\n",
    "labels = review_samples['useful']\n",
    "# labels = tf.keras.utils.to_categorical(labels, num_classes=2, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3837f6dc",
   "metadata": {
    "id": "9a255b76"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "646ab5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 450, 32)           224000    \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 450, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 225, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 7200)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 250)               1800250   \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,027,605\n",
      "Trainable params: 2,027,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Specify model hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "num_filters = [2, 2, 2]\n",
    "kernel_sizes = [2, 3, 4]\n",
    "dense_layer_dims = [10, 4]\n",
    "dropout_rate = 0.7\n",
    "num_classes = len(np.unique(labels, axis=0))\n",
    "\n",
    "\n",
    "# Build Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0d1eee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34/34 - 4s - loss: 0.5639 - accuracy: 0.7435 - val_loss: 0.6889 - val_accuracy: 0.5681 - 4s/epoch - 118ms/step\n",
      "Epoch 2/10\n",
      "34/34 - 4s - loss: 0.3066 - accuracy: 0.9062 - val_loss: 0.8302 - val_accuracy: 0.5746 - 4s/epoch - 106ms/step\n",
      "Epoch 3/10\n",
      "34/34 - 4s - loss: 0.1089 - accuracy: 0.9764 - val_loss: 0.9741 - val_accuracy: 0.5597 - 4s/epoch - 111ms/step\n",
      "Epoch 4/10\n",
      "34/34 - 4s - loss: 0.0446 - accuracy: 0.9937 - val_loss: 1.1107 - val_accuracy: 0.5644 - 4s/epoch - 115ms/step\n",
      "Epoch 5/10\n",
      "34/34 - 4s - loss: 0.0248 - accuracy: 0.9986 - val_loss: 1.2140 - val_accuracy: 0.5812 - 4s/epoch - 126ms/step\n",
      "Epoch 6/10\n",
      "34/34 - 4s - loss: 0.0129 - accuracy: 0.9991 - val_loss: 1.3502 - val_accuracy: 0.5522 - 4s/epoch - 122ms/step\n",
      "Epoch 7/10\n",
      "34/34 - 4s - loss: 0.0110 - accuracy: 0.9991 - val_loss: 1.3260 - val_accuracy: 0.5494 - 4s/epoch - 109ms/step\n",
      "Epoch 8/10\n",
      "34/34 - 4s - loss: 0.0123 - accuracy: 0.9986 - val_loss: 1.3772 - val_accuracy: 0.5392 - 4s/epoch - 117ms/step\n",
      "Epoch 9/10\n",
      "34/34 - 4s - loss: 0.0167 - accuracy: 0.9984 - val_loss: 1.5799 - val_accuracy: 0.5513 - 4s/epoch - 115ms/step\n",
      "Epoch 10/10\n",
      "34/34 - 4s - loss: 0.0062 - accuracy: 0.9993 - val_loss: 1.5240 - val_accuracy: 0.5737 - 4s/epoch - 107ms/step\n",
      "Accuracy: 57.37%\n"
     ]
    }
   ],
   "source": [
    "# Fitting the data onto model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128, verbose=2)\n",
    "# Getting score metrics from our model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# Displays the accuracy of correct sentiment prediction over test data\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a65fd8",
   "metadata": {},
   "source": [
    "### CNN setup from assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1bcf288c",
   "metadata": {
    "id": "1bcf288c"
   },
   "outputs": [],
   "source": [
    "# Specify model hyperparameters\n",
    "epochs = 5\n",
    "embed_dim = 5\n",
    "num_filters = [2, 2, 2]\n",
    "kernel_sizes = [2, 3, 4]\n",
    "dense_layer_dims = [10, 4]\n",
    "dropout_rate = 0.7\n",
    "num_classes = len(np.unique(labels, axis=0))\n",
    "\n",
    "# Construct the convolutional neural network.\n",
    "# The form of each keras layer function is as follows:\n",
    "#    result = keras.layers.LayerType(arguments for the layer)(layer(s) it should use as input)\n",
    "# concretely,\n",
    "#    this_layer_output = keras.layers.Dense(100, activation='relu')(prev_layer_vector)\n",
    "# performs this_layer_output = relu(prev_layer_vector x W + b) where W has 100 columns.\n",
    "\n",
    "# Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
    "# In our case, we are accepting a list of wordids (padded out to max_len).\n",
    "wordids = keras.layers.Input(shape=(max_len,))\n",
    "\n",
    "# Embed the wordids.\n",
    "# Recall, this is just a mathematically equivalent operation to a linear layer and a one-hot\n",
    "h = keras.layers.Embedding(max_words, embed_dim, input_length=max_len)(wordids)\n",
    "\n",
    "# Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
    "# With the default hyperparameters, we construct 2 filters each of size 2, 3, 4.  As in the image above, each filter\n",
    "# is wide enough to span the whole word embedding (this is why the convolution is \"1d\" as seen in the\n",
    "# function name below).\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
    "    conv_layer = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "# Concat the feature maps from each different size.\n",
    "h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "\n",
    "# Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values\n",
    "# in the vector.\n",
    "# See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf for details.\n",
    "h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6e0a135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a5f5b",
   "metadata": {},
   "source": [
    "## X.X Explanation transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5b3cba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.t5 import T5Model\n",
    "from transformers import TFT5Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6f147ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/t5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /Users/dmiller/.cache/huggingface/transformers/tmp_nunrfgi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c1ba793281403c9812f254793e5b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/t5-small/resolve/main/config.json in cache at /Users/dmiller/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "creating metadata file for /Users/dmiller/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/dmiller/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "https://huggingface.co/t5-small/resolve/main/tf_model.h5 not found in cache or force_download set to True, downloading to /Users/dmiller/.cache/huggingface/transformers/tmppq556s3w\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dd04f66e7d44f7ad888e211f283331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/t5-small/resolve/main/tf_model.h5 in cache at /Users/dmiller/.cache/huggingface/transformers/51663d3eebce1656ebbf9cb26c16e243c19f861394299c088496b86b32ef4831.a6c374775a2dd6a6843e2ada3202ba6acae7cf400a3b1bfdca2ec2341e669716.h5\n",
      "creating metadata file for /Users/dmiller/.cache/huggingface/transformers/51663d3eebce1656ebbf9cb26c16e243c19f861394299c088496b86b32ef4831.a6c374775a2dd6a6843e2ada3202ba6acae7cf400a3b1bfdca2ec2341e669716.h5\n",
      "loading weights file https://huggingface.co/t5-small/resolve/main/tf_model.h5 from cache at /Users/dmiller/.cache/huggingface/transformers/51663d3eebce1656ebbf9cb26c16e243c19f861394299c088496b86b32ef4831.a6c374775a2dd6a6843e2ada3202ba6acae7cf400a3b1bfdca2ec2341e669716.h5\n",
      "2022-03-19 16:05:33.679479: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFT5Model.\n",
      "\n",
      "All the layers of TFT5Model were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"save_eval_checkpoints\": True,\n",
    "    \"save_steps\": -1,\n",
    "    \"use_multiprocessing\": False,\n",
    "    \"evaluate_during_training\": True,\n",
    "    \"evaluate_during_training_steps\": 15000,\n",
    "    \"evaluate_during_training_verbose\": True,\n",
    "    \"fp16\": False,\n",
    "\n",
    "    \"wandb_project\": \"Question Generation with T5\",\n",
    "}\n",
    "\n",
    "model = TFT5Model.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b66424",
   "metadata": {},
   "source": [
    "### X.X Pre-trained sentiment span extraction model\n",
    "\n",
    "https://huggingface.co/mrm8488/t5-base-finetuned-span-sentiment-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a18c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = review_samples.iloc[2]['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "084b4b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13b26247a28476582960ccf6baf8b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc75d03e9054f618f8c482bd447a6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e34a1b4819489696e3d2d03186214a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d46c375d564acfbfadf76305001d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817f9f0fc6294e2194a96e78ee161819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'i love you!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-span-sentiment-extraction\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-span-sentiment-extraction\")\n",
    "\n",
    "def get_sentiment_span(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)  # Batch size 1\n",
    "\n",
    "    generated_ids = model.generate(input_ids=input_ids, num_beams=1, max_length=80).squeeze()\n",
    "\n",
    "    predicted_span = tokenizer.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    return predicted_span\n",
    "  \n",
    "get_sentiment_span(\"question: negative context: My bike was put on hold...should have known that.... argh total bummer\")\n",
    "\n",
    "# output: 'argh total bummer'\n",
    "\n",
    "get_sentiment_span(\"question: positive context: On the monday, so i wont be able to be with you! i love you\")\n",
    "\n",
    "# output: 'i love you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9386f8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biggest kick in the rear end, is that I could only get 10 of the CDs now, because I had the first issue and the second issue was too close to shipping.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment_span(\"question: negative context: {}\".format(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19797147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Please understand that I have been a subscriber to Maximum PC Magazine for YEARS. I think it is probably one of the most entertaining Tech Magazines available, and the Disks that come with this little gem are absolutely packed full of the best freeware/shareware available anywhere! So imagine my excitement when I saw this offer. 12 issues of Maximum PC with what I assumed had to be the CDs because of the cover price listed and the savings. That was incredible. NO it wasn't. I ordered this and found out that this is the PRINT ONLY edition. NO CD's. Upon calling Maximum PC they said it would be another [...] per month for the CDs but the biggest kick in the rear end, is that I could only get 10 of the CDs now, because I had the first issue and the second issue was too close to shipping. IF I JUST WANTED THE MAGAZINE I COULD HAVE JUST DOWNLOADED THE FREE PDF'S THAT MAXIMUM PC MAKES AVAILABLE ON THERE WEBSITE. I WANTED BOTH. BTW AMAZON CUSTOMER SERVICE REFUSES TO DO ANYTHING ABOUT THIS OTHER THAN CANCEL THE SUBSCRIPTION OUTRIGHT. DO NOT DO IT. YOU ARE BETTER OFF JUST ORDERING IT DIRECT FROM MAXIMUM PC.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_samples.iloc[2]['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c98f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
