{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbf5cfd",
   "metadata": {},
   "source": [
    "# Amazon Book Reviews NLP\n",
    "\n",
    "### Predicting usefulness and generating justification summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7fd94",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "__Background__\n",
    "\n",
    "__Problem Statement__\n",
    "\n",
    "__Hypothesis__\n",
    "\n",
    "__Data__\n",
    "\n",
    "__Experimental Approach__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea4e58",
   "metadata": {},
   "source": [
    "__Notes__\n",
    "\n",
    "1. Consider using a Colab notebook rather than GCP\n",
    "    1. Need to request/configure the notebook to use a GPU\n",
    "    1. Careful they will ask why you're using it if you don't need it\n",
    "1. Decent resources for sentiment code examples\n",
    "    1. [Amazon Reviews NLP GitHub](https://github.com/louiefb/amazon-reviews-nlp/blob/master/Amazon%20Reviews%20NLP.ipynbz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e4541",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0320af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abac3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency, norm, skew, kurtosis\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from common import vocabulary\n",
    "\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "# import spacy\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from collections import Counter\n",
    "# from wordcloud import WordCloud\n",
    "from unicodedata import normalize\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, Dense, LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f48539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "730ed2db",
   "metadata": {},
   "source": [
    "### 2.1 Load data using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef72912",
   "metadata": {},
   "outputs": [],
   "source": [
    "mags_meta = spark.read.json('data/magazine_subscriptions_meta.json')\n",
    "mags = spark.read.json('data/magazine_subscriptions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7975d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = spark.read.json('data/books.json')\n",
    "books_meta = spark.read.json('data/books_meta.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mags.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b6e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mags_meta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ee427",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_meta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b68c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_meta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f5e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_categories = mags_meta.groupBy('category')\n",
    "mag_categories.count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mags_pd = mags.toPandas()\n",
    "mags_meta_pd = mags_meta.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6468f6",
   "metadata": {},
   "source": [
    "### 2.2 Load data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1555e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - runtime is untenable on the books data in local jupyter notebook\n",
    "\n",
    "# books_meta = pd.read_json('data/books_meta.json', lines=True)\n",
    "# books = pd.read_json('data/books.json', lines=True)\n",
    "meta = pd.read_json('data/magazine_subscriptions_meta.json', lines=True)\n",
    "reviews = pd.read_json('data/magazine_subscriptions.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0625c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate records of a unique ASIN\n",
    "meta.drop_duplicates(subset='asin', inplace=True)\n",
    "\n",
    "# Drop reviews with no reviewText\n",
    "reviews = reviews[reviews['reviewText'].isna() == False]\n",
    "\n",
    "# Convert vote column to numeric\n",
    "reviews['vote'] = reviews['vote'].str.replace(',', '').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\" + \"Dataframe Shape\" + \"\\033[0m\")\n",
    "print(reviews.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\033[1m\" + \"Column Information\" + \"\\033[0m\")\n",
    "reviews.info()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['reviewText'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f81714",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\" + \"Dataframe Shape\" + \"\\033[0m\")\n",
    "print(meta.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\033[1m\" + \"Column Information\" + \"\\033[0m\")\n",
    "meta.info()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135802f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.iloc[0].category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674096a0",
   "metadata": {},
   "source": [
    "### 2.3 Meta data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['category', 'subcat1', 'subcat2', 'subcat3', 'subcat4']\n",
    "meta[categories] = pd.DataFrame(meta['category'].to_list())\n",
    "meta.replace('amp;', '', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e5fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['category'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcat = 'subcat1'\n",
    "meta[subcat].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['brand'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_if(group):\n",
    "    '''Count the values of a boolean column that are True'''\n",
    "    return np.sum(group==True)\n",
    "\n",
    "# Use groupby and agg to summarize relevant statistics from review data by each ASIN\n",
    "asin_stats = reviews.groupby('asin').agg({'asin': 'count',\n",
    "                                          'overall': 'mean',\n",
    "                                          'verified': count_if                                         \n",
    "                                         })\n",
    "\n",
    "# Rename columns and reset index\n",
    "asin_stats.rename(columns={'asin':'reviews',\n",
    "                           'overall':'avgRating',\n",
    "                           'verified': 'reviewsVerified'\n",
    "                          },\n",
    "                  inplace=True\n",
    "                 )\n",
    "asin_stats.reset_index(inplace=True)\n",
    "\n",
    "# Merge ASIN statistics df with original metadata, drop irrelevant columns\n",
    "asins = pd.merge(meta,\n",
    "                asin_stats,\n",
    "                on='asin',\n",
    "                how='left'\n",
    "               )\n",
    "\n",
    "asins = asins[['asin', 'subcat1', 'brand', 'reviews', 'reviewsVerified', 'avgRating']]\n",
    "asins['pctVerified'] = asins['reviewsVerified'] / asins['reviews']\n",
    "asins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\" + \"Column Information\" + \"\\033[0m\")\n",
    "print(asins.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71eb6f",
   "metadata": {},
   "source": [
    "### 2.4 Meta data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='reviews', y='reviewsVerified', data=asins, hue='subcat1', fit_reg=False)\\\n",
    "   .set(title='Reviews vs. Verified Reviews by SubCategory')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e67e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,12))\n",
    "sns.boxplot(x='avgRating', y='subcat1', orient='h', data=asins)\n",
    "# sns.swarmplot(x='avgRating', y='subcat1', orient='h', data=asins, color='.1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7eb31",
   "metadata": {},
   "source": [
    "### 2.5 Filter and random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ASINs to reviews >= 10\n",
    "asin_samples = asins[asins['reviews'] >= 10]\n",
    "\n",
    "# Filter ASINs to subcat1 >= 10\n",
    "# Need to consider whether I want to do this or not\n",
    "# cat_counts = asin_samples['subcat1'].value_counts(dropna=False)\n",
    "# asin_samples = asin_samples.loc[asin_samples['subcat1'].isin(cat_counts[cat_counts >= 30].index), :]\n",
    "\n",
    "asin_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c06277",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='reviews', y='reviewsVerified', data=asin_samples, hue='subcat1', fit_reg=True)\\\n",
    "   .set(title='Reviews vs. Verified Reviews by SubCategory')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter reviews based on asin_samples\n",
    "review_samples = pd.merge(reviews,\n",
    "                          asin_samples,\n",
    "                          on='asin',\n",
    "                          how='inner'\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a65227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample 5 reviews from each ASIN\n",
    "# This is likely over-sampling - DON'T USE \n",
    "# review_samples = review_samples.groupby('asin').sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews.shape)\n",
    "print(review_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f99f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers greater than 3 std-dev above mean votes for usefulness\n",
    "# This makes visualization more interpretable, but likely not a good idea for modeling\n",
    "# Because we want the outliers to be considered the \"most useful\"\n",
    "\n",
    "# outlier_threshold = review_samples.describe()['vote']['mean'] +\\\n",
    "#                     (review_samples.describe()['vote']['std'] * 3)\n",
    "\n",
    "# review_samples = review_samples[review_samples['vote'] <= outlier_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a88d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove reviews with X votes\n",
    "\n",
    "min_votes = 10\n",
    "review_samples = review_samples[review_samples['vote'] >= min_votes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews.shape)\n",
    "print(review_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='avgRating', y='vote', data=review_samples, hue='subcat1', fit_reg=True)\\\n",
    "   .set(title='Reviews vs. Verified Reviews by SubCategory')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185662a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,12))\n",
    "sns.boxplot(x='vote', y='subcat1', hue='verified', orient='h', data=review_samples)\n",
    "# sns.swarmplot(x='vote', y='subcat1', hue='verified', orient='h', data=review_samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65faf890",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='vote',\n",
    "             hue='verified',\n",
    "             data=review_samples[review_samples['vote'] < 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8890a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='vote',\n",
    "             hue='verified',\n",
    "             data=review_samples[(review_samples['vote'] >= 10) & (review_samples['vote'] < 100)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='vote',\n",
    "             hue='verified',\n",
    "             data=review_samples[review_samples['vote'] >= 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore skew and kurtosis functions in pandas and scipy\n",
    "pd_skew    = review_samples['vote'].skew()\n",
    "pd_kurt    = review_samples['vote'].kurt()\n",
    "sci_skew   = skew(review_samples['vote'], bias=False)\n",
    "sci_kurt   = kurtosis(review_samples['vote'], bias=False)\n",
    "sci_skew_b = skew(review_samples['vote'], bias=True)\n",
    "sci_kurt_b = kurtosis(review_samples['vote'], bias=True)\n",
    "\n",
    "pd.DataFrame({'skew': [pd_skew, sci_skew, sci_skew_b],\n",
    "              'kurtosis': [pd_kurt, sci_kurt, sci_kurt_b]},\n",
    "             index=['pandas', 'scipy-unbiased', 'scipy-biased']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f725b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize vote variable\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Take the log10 of votes\n",
    "review_samples['vote_norm'] = boxcox(review_samples['vote'], lmbda=0)\n",
    "\n",
    "# Apply power transformation\n",
    "pt = PowerTransformer()\n",
    "review_samples['vote_power'] = pt.fit_transform(pd.DataFrame(review_samples['vote']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787498bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(20,10))\n",
    "\n",
    "sns.histplot(x='vote',\n",
    "             data=review_samples,\n",
    "             bins=20,\n",
    "             ax=ax[0][0]\n",
    "            )\n",
    "\n",
    "sns.histplot(x='vote_norm',\n",
    "             data=review_samples,\n",
    "             bins=20,\n",
    "             ax=ax[0][1]\n",
    "            )\n",
    "\n",
    "sns.histplot(x='vote_power',\n",
    "             data=review_samples,\n",
    "             bins=20,\n",
    "             ax=ax[1][0]\n",
    "            )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the usefulness classification based on vote_power variable\n",
    "\n",
    "review_samples['useful'] = review_samples['vote_power'].apply(lambda x: 1 if x > 0 else 0)\n",
    "review_samples['useful'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_samples.to_csv('data/review_samples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7524a67",
   "metadata": {},
   "source": [
    "## 3. Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use html library to decode special characters\n",
    "html_reviews = review_samples[review_samples['reviewText'].str.contains('&#', na=False)]\n",
    "\n",
    "if html_reviews.empty:\n",
    "    # Doesn't appear to be an issue in this data, but will use the else statement to check\n",
    "    pass\n",
    "else:\n",
    "    # Show an example of the unescape function\n",
    "    sample_review = html_reviews['reviewText'].iloc[0]\n",
    "    print(sample_review)\n",
    "    print(\"\")\n",
    "    decoded_review = html.unescape(sample_review)\n",
    "    print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e109df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "pattern = r\"\\&\\#[0-9]+\\;\"\n",
    "\n",
    "review_samples[\"reviewText\"] = review_samples[\"reviewText\"].str.replace(pat=pattern, repl=\"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9304e9",
   "metadata": {},
   "source": [
    "### 3.1 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b78a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lemmatization to reduce words to their root form\n",
    "\n",
    "# import nltk resources\n",
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create Lemmatizer object\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(tagged_token):\n",
    "    \"\"\" Returns lemmatized word given its tag\"\"\"\n",
    "    root = []\n",
    "    for token in tagged_token:\n",
    "        tag = token[1][0]\n",
    "        word = token[0]\n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    return root\n",
    "\n",
    "def lemmatize_doc(document):\n",
    "    \"\"\" Tags words then returns sentence with lemmatized words\"\"\"\n",
    "    lemmatized_list = []\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "    for sentence in tokenized_sent:\n",
    "        no_punctuation = re.sub(r\"[`'\\\",.!?()]\", \" \", sentence)\n",
    "        tokenized_word = word_tokenize(no_punctuation)\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        lemmatized = lemmatize_word(tagged_token)\n",
    "        lemmatized_list.extend(lemmatized)\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "# apply functions to review text\n",
    "review_samples[\"reviewProcessed\"] = review_samples[\"reviewText\"].apply(lambda row: lemmatize_doc(row))\n",
    "\n",
    "# Example review\n",
    "print(review_samples[\"reviewText\"].iloc[1])\n",
    "print(\"\")\n",
    "print(review_samples[\"reviewProcessed\"].iloc[1])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554ce4d",
   "metadata": {},
   "source": [
    "### 3.2 General normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove accents\n",
    "\n",
    "remove_accent = lambda text: normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "review_samples[\"reviewProcessed\"] = review_samples[\"reviewProcessed\"].apply(remove_accent)\n",
    "\n",
    "# Remove punctuation\n",
    "pattern = r\"[^\\w\\s]\"\n",
    "review_samples[\"reviewProcessed\"] = review_samples[\"reviewProcessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "# Convert to lowercase\n",
    "review_samples[\"reviewProcessed\"] = review_samples[\"reviewProcessed\"].str.lower()\n",
    "\n",
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words = [word.replace(\"\\'\", \"\") for word in stop_words]\n",
    "# print(f\"sample stop words: {stop_words[:15]} \\n\")\n",
    "\n",
    "remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \") if token not in stop_words])\n",
    "review_samples[\"reviewProcessed\"] = review_samples[\"reviewProcessed\"].apply(remove_stop_words)\n",
    "\n",
    "# Remove extra spaces\n",
    "pattern = r\"[\\s]+\"\n",
    "review_samples[\"reviewProcessed\"] = review_samples[\"reviewProcessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "# Example review\n",
    "print(review_samples[\"reviewText\"].iloc[1])\n",
    "print(\"\")\n",
    "print(review_samples[\"reviewProcessed\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ae8ee",
   "metadata": {},
   "source": [
    "### 3.3 Tokenization and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpora\n",
    "corpora = review_samples['reviewText'].values\n",
    "tokenized = [corpus.split(\" \") for corpus in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b223732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup phrase modeling\n",
    "bi_gram = Phrases(tokenized, min_count=300, threshold=50)\n",
    "\n",
    "tri_gram = Phrases(bi_gram[tokenized], min_count=50, threshold=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7727ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigrams\n",
    "uni_gram_tokens = set([token for text in tokenized for token in text])\n",
    "uni_gram_tokens = set(filter(lambda x: x != \"\", uni_gram_tokens))\n",
    "\n",
    "print(list(uni_gram_tokens)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7611a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams\n",
    "bigram_min = bi_gram.min_count\n",
    "bi_condition = lambda x: x[1] >= bigram_min\n",
    "\n",
    "bi_gram_tokens = dict(filter(bi_condition, bi_gram.vocab.items()))\n",
    "bi_gram_tokens = set(bi_gram_tokens)\n",
    "\n",
    "bi_grams_only = bi_gram_tokens.difference(uni_gram_tokens)\n",
    "print(list(bi_grams_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams \n",
    "trigram_min = tri_gram.min_count\n",
    "\n",
    "tri_condition = lambda x: x[1] >= trigram_min\n",
    "\n",
    "tri_gram_tokens = dict(filter(tri_condition, tri_gram.vocab.items()))\n",
    "tri_gram_tokens = set(tri_gram_tokens)\n",
    "\n",
    "tri_grams_only = tri_gram_tokens.difference(bi_gram_tokens)\n",
    "print(list(tri_grams_only)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbfaff0",
   "metadata": {},
   "source": [
    "## 4.0 Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ac3c5",
   "metadata": {},
   "source": [
    "### 4.1 Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ab981",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "max_len = 200\n",
    "\n",
    "reviews = review_samples['reviewText']\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "reviews = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "labels = review_samples['useful']\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=2, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a255b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6556cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model hyperparameters.\n",
    "epochs = 5\n",
    "dropout_rate = 0.7\n",
    "num_classes = len(np.unique(labels, axis=0))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 40, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(20, dropout=dropout_rate)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4bcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test.argmax(axis=1)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d45011",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true,\n",
    "                                        y_pred,\n",
    "                                        normalize='all',\n",
    "                                        display_labels=sentiment\n",
    "                                       )\n",
    "\n",
    "plt.title(\"Confusion Matrix of Review Usefulness\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59598ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b299f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['i really loved how the technician helped me with the issue that i had'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73365d",
   "metadata": {},
   "source": [
    "### 4.X BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a87b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = review_samples['reviewText'], review_samples['useful']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
    "\n",
    "train = pd.DataFrame([X_train, y_train]).T\n",
    "dev = pd.DataFrame([X_dev, y_dev]).T\n",
    "test = pd.DataFrame([X_test, y_test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(data, DATA_COLUMN, LABEL_COLUMN): \n",
    "    examples = data.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                 text_a = x[DATA_COLUMN], \n",
    "                                                 text_b = None,\n",
    "                                                 label = x[LABEL_COLUMN]\n",
    "                                                ),\n",
    "                          axis = 1\n",
    "                         )\n",
    "  \n",
    "    return examples\n",
    "\n",
    "\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(e.text_a,\n",
    "                                           add_special_tokens=True,\n",
    "                                           max_length=max_length, # truncates if len(s) > max_length\n",
    "                                           return_token_type_ids=True,\n",
    "                                           return_attention_mask=True,\n",
    "                                           pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "                                           truncation=True\n",
    "                                          )\n",
    "\n",
    "        input_ids = input_dict[\"input_ids\"]\n",
    "        token_type_ids = input_dict[\"token_type_ids\"] \n",
    "        attention_mask = input_dict['attention_mask']\n",
    "\n",
    "        features.append(InputFeatures(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask,\n",
    "                                      token_type_ids=token_type_ids,\n",
    "                                      label=e.label\n",
    "                                     )\n",
    "                       )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield ({\"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                   },\n",
    "                   f.label,\n",
    "                  )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen,\n",
    "                                          ({\"input_ids\": tf.int32,\n",
    "                                            \"attention_mask\": tf.int32,\n",
    "                                            \"token_type_ids\": tf.int32\n",
    "                                           },\n",
    "                                           tf.int64\n",
    "                                          ),\n",
    "                                          ({\"input_ids\": tf.TensorShape([None]),\n",
    "                                            \"attention_mask\": tf.TensorShape([None]),\n",
    "                                            \"token_type_ids\": tf.TensorShape([None]),\n",
    "                                           },\n",
    "                                           tf.TensorShape([]),\n",
    "                                          ),\n",
    "                                         )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'reviewText'\n",
    "LABEL_COLUMN = 'useful'\n",
    "\n",
    "train_InputExamples = convert_data_to_examples(test, DATA_COLUMN, LABEL_COLUMN)\n",
    "dev_InputExamples = convert_data_to_examples(test, DATA_COLUMN, LABEL_COLUMN)\n",
    "test_InputExamples = convert_data_to_examples(test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "dev_data = convert_examples_to_tf_dataset(list(dev_InputExamples), tokenizer)\n",
    "dev_data = dev_data.batch(32)\n",
    "\n",
    "test_data = convert_examples_to_tf_dataset(list(test_InputExamples), tokenizer)\n",
    "test_data = test_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=2, validation_data=dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549dde8",
   "metadata": {},
   "source": [
    "### 4.X Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model hyperparameters\n",
    "epochs = 5\n",
    "embed_dim = 5\n",
    "num_filters = [2, 2, 2]\n",
    "kernel_sizes = [2, 3, 4]\n",
    "dense_layer_dims = [10, 4]\n",
    "dropout_rate = 0.7\n",
    "num_classes = len(np.unique(labels, axis=0))\n",
    "\n",
    "# Construct the convolutional neural network.\n",
    "# The form of each keras layer function is as follows:\n",
    "#    result = keras.layers.LayerType(arguments for the layer)(layer(s) it should use as input)\n",
    "# concretely,\n",
    "#    this_layer_output = keras.layers.Dense(100, activation='relu')(prev_layer_vector)\n",
    "# performs this_layer_output = relu(prev_layer_vector x W + b) where W has 100 columns.\n",
    "\n",
    "# Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
    "# In our case, we are accepting a list of wordids (padded out to max_len).\n",
    "wordids = keras.layers.Input(shape=(max_len,))\n",
    "\n",
    "# Embed the wordids.\n",
    "# Recall, this is just a mathematically equivalent operation to a linear layer and a one-hot\n",
    "h = keras.layers.Embedding(ds.vocab.size, embed_dim, input_length=max_len)(wordids)\n",
    "\n",
    "# Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
    "# With the default hyperparameters, we construct 2 filters each of size 2, 3, 4.  As in the image above, each filter\n",
    "# is wide enough to span the whole word embedding (this is why the convolution is \"1d\" as seen in the\n",
    "# function name below).\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "\n",
    "# Concat the feature maps from each different size.\n",
    "h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
    "\n",
    "# Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values\n",
    "# in the vector.\n",
    "# See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf for details.\n",
    "h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Add a fully connected layer for each dense layer dimension in dense_layer_dims.\n",
    "### END YOUR CODE\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=wordids, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2c80d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
